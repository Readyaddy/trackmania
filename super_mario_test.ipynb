{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pip in d:\\mini_conda\\envs\\collabkart\\lib\\site-packages (23.3.1)\n",
      "Collecting pip\n",
      "  Downloading pip-24.2-py3-none-any.whl.metadata (3.6 kB)\n",
      "Requirement already satisfied: setuptools in d:\\mini_conda\\envs\\collabkart\\lib\\site-packages (68.2.2)\n",
      "Collecting setuptools\n",
      "  Using cached setuptools-72.2.0-py3-none-any.whl.metadata (6.6 kB)\n",
      "Requirement already satisfied: wheel in d:\\mini_conda\\envs\\collabkart\\lib\\site-packages (0.41.2)\n",
      "Collecting wheel\n",
      "  Using cached wheel-0.44.0-py3-none-any.whl.metadata (2.3 kB)\n",
      "Downloading pip-24.2-py3-none-any.whl (1.8 MB)\n",
      "   ---------------------------------------- 0.0/1.8 MB ? eta -:--:--\n",
      "    --------------------------------------- 0.0/1.8 MB 1.4 MB/s eta 0:00:02\n",
      "   --- ------------------------------------ 0.2/1.8 MB 2.6 MB/s eta 0:00:01\n",
      "   ----------- ---------------------------- 0.5/1.8 MB 4.7 MB/s eta 0:00:01\n",
      "   -------------------- ------------------- 0.9/1.8 MB 5.8 MB/s eta 0:00:01\n",
      "   ---------------------------- ----------- 1.3/1.8 MB 6.4 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 1.7/1.8 MB 6.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------  1.8/1.8 MB 6.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 1.8/1.8 MB 6.1 MB/s eta 0:00:00\n",
      "Using cached setuptools-72.2.0-py3-none-any.whl (2.3 MB)\n",
      "Using cached wheel-0.44.0-py3-none-any.whl (67 kB)\n",
      "Installing collected packages: wheel, setuptools, pip\n",
      "  Attempting uninstall: wheel\n",
      "    Found existing installation: wheel 0.41.2\n",
      "    Uninstalling wheel-0.41.2:\n",
      "      Successfully uninstalled wheel-0.41.2\n",
      "  Attempting uninstall: setuptools\n",
      "    Found existing installation: setuptools 68.2.2\n",
      "    Uninstalling setuptools-68.2.2:\n",
      "      Successfully uninstalled setuptools-68.2.2\n",
      "  Attempting uninstall: pip\n",
      "    Found existing installation: pip 23.3.1\n",
      "    Uninstalling pip-23.3.1:\n",
      "      Successfully uninstalled pip-23.3.1\n",
      "Successfully installed pip-24.2 setuptools-72.2.0 wheel-0.44.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install --upgrade pip setuptools wheel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting nes-py\n",
      "  Using cached nes_py-8.2.1.tar.gz (77 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Requirement already satisfied: gym>=0.17.2 in d:\\mini_conda\\envs\\collabkart\\lib\\site-packages (from nes-py) (0.26.2)\n",
      "Requirement already satisfied: numpy>=1.18.5 in d:\\mini_conda\\envs\\collabkart\\lib\\site-packages (from nes-py) (1.26.3)\n",
      "Collecting pyglet<=1.5.21,>=1.4.0 (from nes-py)\n",
      "  Using cached pyglet-1.5.21-py3-none-any.whl.metadata (7.6 kB)\n",
      "Requirement already satisfied: tqdm>=4.48.2 in d:\\mini_conda\\envs\\collabkart\\lib\\site-packages (from nes-py) (4.66.1)\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in d:\\mini_conda\\envs\\collabkart\\lib\\site-packages (from gym>=0.17.2->nes-py) (3.0.0)\n",
      "Requirement already satisfied: gym-notices>=0.0.4 in d:\\mini_conda\\envs\\collabkart\\lib\\site-packages (from gym>=0.17.2->nes-py) (0.0.8)\n",
      "Requirement already satisfied: colorama in d:\\mini_conda\\envs\\collabkart\\lib\\site-packages (from tqdm>=4.48.2->nes-py) (0.4.6)\n",
      "Using cached pyglet-1.5.21-py3-none-any.whl (1.1 MB)\n",
      "Building wheels for collected packages: nes-py\n",
      "  Building wheel for nes-py (setup.py): started\n",
      "  Building wheel for nes-py (setup.py): finished with status 'error'\n",
      "  Running setup.py clean for nes-py\n",
      "Failed to build nes-py\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  error: subprocess-exited-with-error\n",
      "  \n",
      "  × python setup.py bdist_wheel did not run successfully.\n",
      "  │ exit code: 1\n",
      "  ╰─> [21 lines of output]\n",
      "      running bdist_wheel\n",
      "      running build\n",
      "      running build_py\n",
      "      creating build\n",
      "      creating build\\lib.win-amd64-cpython-311\n",
      "      creating build\\lib.win-amd64-cpython-311\\nes_py\n",
      "      copying nes_py\\nes_env.py -> build\\lib.win-amd64-cpython-311\\nes_py\n",
      "      copying nes_py\\_image_viewer.py -> build\\lib.win-amd64-cpython-311\\nes_py\n",
      "      copying nes_py\\_rom.py -> build\\lib.win-amd64-cpython-311\\nes_py\n",
      "      copying nes_py\\__init__.py -> build\\lib.win-amd64-cpython-311\\nes_py\n",
      "      creating build\\lib.win-amd64-cpython-311\\nes_py\\app\n",
      "      copying nes_py\\app\\cli.py -> build\\lib.win-amd64-cpython-311\\nes_py\\app\n",
      "      copying nes_py\\app\\play_human.py -> build\\lib.win-amd64-cpython-311\\nes_py\\app\n",
      "      copying nes_py\\app\\play_random.py -> build\\lib.win-amd64-cpython-311\\nes_py\\app\n",
      "      copying nes_py\\app\\__init__.py -> build\\lib.win-amd64-cpython-311\\nes_py\\app\n",
      "      creating build\\lib.win-amd64-cpython-311\\nes_py\\wrappers\n",
      "      copying nes_py\\wrappers\\joypad_space.py -> build\\lib.win-amd64-cpython-311\\nes_py\\wrappers\n",
      "      copying nes_py\\wrappers\\__init__.py -> build\\lib.win-amd64-cpython-311\\nes_py\\wrappers\n",
      "      running build_ext\n",
      "      building 'nes_py.lib_nes_env' extension\n",
      "      error: Microsoft Visual C++ 14.0 or greater is required. Get it with \"Microsoft C++ Build Tools\": https://visualstudio.microsoft.com/visual-cpp-build-tools/\n",
      "      [end of output]\n",
      "  \n",
      "  note: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "  ERROR: Failed building wheel for nes-py\n",
      "ERROR: ERROR: Failed to build installable wheels for some pyproject.toml based projects (nes-py)\n"
     ]
    }
   ],
   "source": [
    "pip install nes-py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting gym-super-mario-bros\n",
      "  Using cached gym_super_mario_bros-7.4.0-py3-none-any.whl.metadata (10 kB)\n",
      "Requirement already satisfied: torch in d:\\mini_conda\\envs\\collabkart\\lib\\site-packages (2.1.2+cu121)\n",
      "Requirement already satisfied: numpy in d:\\mini_conda\\envs\\collabkart\\lib\\site-packages (1.26.3)\n",
      "Collecting nes-py>=8.1.4 (from gym-super-mario-bros)\n",
      "  Using cached nes_py-8.2.1.tar.gz (77 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Requirement already satisfied: filelock in d:\\mini_conda\\envs\\collabkart\\lib\\site-packages (from torch) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions in d:\\mini_conda\\envs\\collabkart\\lib\\site-packages (from torch) (4.9.0)\n",
      "Requirement already satisfied: sympy in d:\\mini_conda\\envs\\collabkart\\lib\\site-packages (from torch) (1.12)\n",
      "Requirement already satisfied: networkx in d:\\mini_conda\\envs\\collabkart\\lib\\site-packages (from torch) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in d:\\mini_conda\\envs\\collabkart\\lib\\site-packages (from torch) (3.1.3)\n",
      "Requirement already satisfied: fsspec in d:\\mini_conda\\envs\\collabkart\\lib\\site-packages (from torch) (2023.12.2)\n",
      "Requirement already satisfied: gym>=0.17.2 in d:\\mini_conda\\envs\\collabkart\\lib\\site-packages (from nes-py>=8.1.4->gym-super-mario-bros) (0.26.2)\n",
      "Collecting pyglet<=1.5.21,>=1.4.0 (from nes-py>=8.1.4->gym-super-mario-bros)\n",
      "  Using cached pyglet-1.5.21-py3-none-any.whl.metadata (7.6 kB)\n",
      "Requirement already satisfied: tqdm>=4.48.2 in d:\\mini_conda\\envs\\collabkart\\lib\\site-packages (from nes-py>=8.1.4->gym-super-mario-bros) (4.66.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in d:\\mini_conda\\envs\\collabkart\\lib\\site-packages (from jinja2->torch) (2.1.3)\n",
      "Requirement already satisfied: mpmath>=0.19 in d:\\mini_conda\\envs\\collabkart\\lib\\site-packages (from sympy->torch) (1.3.0)\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in d:\\mini_conda\\envs\\collabkart\\lib\\site-packages (from gym>=0.17.2->nes-py>=8.1.4->gym-super-mario-bros) (3.0.0)\n",
      "Requirement already satisfied: gym-notices>=0.0.4 in d:\\mini_conda\\envs\\collabkart\\lib\\site-packages (from gym>=0.17.2->nes-py>=8.1.4->gym-super-mario-bros) (0.0.8)\n",
      "Requirement already satisfied: colorama in d:\\mini_conda\\envs\\collabkart\\lib\\site-packages (from tqdm>=4.48.2->nes-py>=8.1.4->gym-super-mario-bros) (0.4.6)\n",
      "Using cached gym_super_mario_bros-7.4.0-py3-none-any.whl (199 kB)\n",
      "Using cached pyglet-1.5.21-py3-none-any.whl (1.1 MB)\n",
      "Building wheels for collected packages: nes-py\n",
      "  Building wheel for nes-py (setup.py): started\n",
      "  Building wheel for nes-py (setup.py): finished with status 'error'\n",
      "  Running setup.py clean for nes-py\n",
      "Failed to build nes-py\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  error: subprocess-exited-with-error\n",
      "  \n",
      "  × python setup.py bdist_wheel did not run successfully.\n",
      "  │ exit code: 1\n",
      "  ╰─> [21 lines of output]\n",
      "      running bdist_wheel\n",
      "      running build\n",
      "      running build_py\n",
      "      creating build\n",
      "      creating build\\lib.win-amd64-cpython-311\n",
      "      creating build\\lib.win-amd64-cpython-311\\nes_py\n",
      "      copying nes_py\\nes_env.py -> build\\lib.win-amd64-cpython-311\\nes_py\n",
      "      copying nes_py\\_image_viewer.py -> build\\lib.win-amd64-cpython-311\\nes_py\n",
      "      copying nes_py\\_rom.py -> build\\lib.win-amd64-cpython-311\\nes_py\n",
      "      copying nes_py\\__init__.py -> build\\lib.win-amd64-cpython-311\\nes_py\n",
      "      creating build\\lib.win-amd64-cpython-311\\nes_py\\app\n",
      "      copying nes_py\\app\\cli.py -> build\\lib.win-amd64-cpython-311\\nes_py\\app\n",
      "      copying nes_py\\app\\play_human.py -> build\\lib.win-amd64-cpython-311\\nes_py\\app\n",
      "      copying nes_py\\app\\play_random.py -> build\\lib.win-amd64-cpython-311\\nes_py\\app\n",
      "      copying nes_py\\app\\__init__.py -> build\\lib.win-amd64-cpython-311\\nes_py\\app\n",
      "      creating build\\lib.win-amd64-cpython-311\\nes_py\\wrappers\n",
      "      copying nes_py\\wrappers\\joypad_space.py -> build\\lib.win-amd64-cpython-311\\nes_py\\wrappers\n",
      "      copying nes_py\\wrappers\\__init__.py -> build\\lib.win-amd64-cpython-311\\nes_py\\wrappers\n",
      "      running build_ext\n",
      "      building 'nes_py.lib_nes_env' extension\n",
      "      error: Microsoft Visual C++ 14.0 or greater is required. Get it with \"Microsoft C++ Build Tools\": https://visualstudio.microsoft.com/visual-cpp-build-tools/\n",
      "      [end of output]\n",
      "  \n",
      "  note: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "  ERROR: Failed building wheel for nes-py\n",
      "ERROR: Could not build wheels for nes-py, which is required to install pyproject.toml-based projects\n"
     ]
    }
   ],
   "source": [
    "pip install gym-super-mario-bros torch numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting gym-super-mario-bros\n",
      "  Using cached gym_super_mario_bros-7.4.0-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting nes-py>=8.1.4 (from gym-super-mario-bros)\n",
      "  Using cached nes_py-8.2.1.tar.gz (77 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Requirement already satisfied: gym>=0.17.2 in d:\\mini_conda\\envs\\collabkart\\lib\\site-packages (from nes-py>=8.1.4->gym-super-mario-bros) (0.26.2)\n",
      "Requirement already satisfied: numpy>=1.18.5 in d:\\mini_conda\\envs\\collabkart\\lib\\site-packages (from nes-py>=8.1.4->gym-super-mario-bros) (1.26.3)\n",
      "Collecting pyglet<=1.5.21,>=1.4.0 (from nes-py>=8.1.4->gym-super-mario-bros)\n",
      "  Using cached pyglet-1.5.21-py3-none-any.whl.metadata (7.6 kB)\n",
      "Requirement already satisfied: tqdm>=4.48.2 in d:\\mini_conda\\envs\\collabkart\\lib\\site-packages (from nes-py>=8.1.4->gym-super-mario-bros) (4.66.1)\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in d:\\mini_conda\\envs\\collabkart\\lib\\site-packages (from gym>=0.17.2->nes-py>=8.1.4->gym-super-mario-bros) (3.0.0)\n",
      "Requirement already satisfied: gym-notices>=0.0.4 in d:\\mini_conda\\envs\\collabkart\\lib\\site-packages (from gym>=0.17.2->nes-py>=8.1.4->gym-super-mario-bros) (0.0.8)\n",
      "Requirement already satisfied: colorama in d:\\mini_conda\\envs\\collabkart\\lib\\site-packages (from tqdm>=4.48.2->nes-py>=8.1.4->gym-super-mario-bros) (0.4.6)\n",
      "Using cached gym_super_mario_bros-7.4.0-py3-none-any.whl (199 kB)\n",
      "Using cached pyglet-1.5.21-py3-none-any.whl (1.1 MB)\n",
      "Building wheels for collected packages: nes-py\n",
      "  Building wheel for nes-py (setup.py): started\n",
      "  Building wheel for nes-py (setup.py): finished with status 'error'\n",
      "  Running setup.py clean for nes-py\n",
      "Failed to build nes-py\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  error: subprocess-exited-with-error\n",
      "  \n",
      "  × python setup.py bdist_wheel did not run successfully.\n",
      "  │ exit code: 1\n",
      "  ╰─> [21 lines of output]\n",
      "      running bdist_wheel\n",
      "      running build\n",
      "      running build_py\n",
      "      creating build\n",
      "      creating build\\lib.win-amd64-cpython-311\n",
      "      creating build\\lib.win-amd64-cpython-311\\nes_py\n",
      "      copying nes_py\\nes_env.py -> build\\lib.win-amd64-cpython-311\\nes_py\n",
      "      copying nes_py\\_image_viewer.py -> build\\lib.win-amd64-cpython-311\\nes_py\n",
      "      copying nes_py\\_rom.py -> build\\lib.win-amd64-cpython-311\\nes_py\n",
      "      copying nes_py\\__init__.py -> build\\lib.win-amd64-cpython-311\\nes_py\n",
      "      creating build\\lib.win-amd64-cpython-311\\nes_py\\app\n",
      "      copying nes_py\\app\\cli.py -> build\\lib.win-amd64-cpython-311\\nes_py\\app\n",
      "      copying nes_py\\app\\play_human.py -> build\\lib.win-amd64-cpython-311\\nes_py\\app\n",
      "      copying nes_py\\app\\play_random.py -> build\\lib.win-amd64-cpython-311\\nes_py\\app\n",
      "      copying nes_py\\app\\__init__.py -> build\\lib.win-amd64-cpython-311\\nes_py\\app\n",
      "      creating build\\lib.win-amd64-cpython-311\\nes_py\\wrappers\n",
      "      copying nes_py\\wrappers\\joypad_space.py -> build\\lib.win-amd64-cpython-311\\nes_py\\wrappers\n",
      "      copying nes_py\\wrappers\\__init__.py -> build\\lib.win-amd64-cpython-311\\nes_py\\wrappers\n",
      "      running build_ext\n",
      "      building 'nes_py.lib_nes_env' extension\n",
      "      error: Microsoft Visual C++ 14.0 or greater is required. Get it with \"Microsoft C++ Build Tools\": https://visualstudio.microsoft.com/visual-cpp-build-tools/\n",
      "      [end of output]\n",
      "  \n",
      "  note: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "  ERROR: Failed building wheel for nes-py\n",
      "ERROR: ERROR: Failed to build installable wheels for some pyproject.toml based projects (nes-py)\n"
     ]
    }
   ],
   "source": [
    "pip install gym-super-mario-bros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'gym_super_mario_bros'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 5\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01moptim\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01moptim\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mgym_super_mario_bros\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgym_super_mario_bros\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mactions\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SIMPLE_MOVEMENT\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgym\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mwrappers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m GrayScaleObservation\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'gym_super_mario_bros'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import gym_super_mario_bros\n",
    "from gym_super_mario_bros.actions import SIMPLE_MOVEMENT\n",
    "from gym.wrappers import GrayScaleObservation\n",
    "from gym.spaces import Box\n",
    "\n",
    "# Define the neural network\n",
    "class DQN(nn.Module):\n",
    "    def __init__(self, input_shape, n_actions):\n",
    "        super(DQN, self).__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(input_shape[0], 32, kernel_size=8, stride=4),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, kernel_size=4, stride=2),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, stride=1),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        conv_out_size = self._get_conv_out(input_shape)\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(conv_out_size, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, n_actions)\n",
    "        )\n",
    "\n",
    "    def _get_conv_out(self, shape):\n",
    "        o = self.conv(torch.zeros(1, *shape))\n",
    "        return int(np.prod(o.size()))\n",
    "\n",
    "    def forward(self, x):\n",
    "        conv_out = self.conv(x).view(x.size()[0], -1)\n",
    "        return self.fc(conv_out)\n",
    "\n",
    "# Setup the environment\n",
    "env = gym_super_mario_bros.make('SuperMarioBros-v0')\n",
    "env = GrayScaleObservation(env, keep_dim=True)\n",
    "env = gym.wrappers.FrameStack(env, 4)\n",
    "\n",
    "# Initialize the DQN\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "n_actions = env.action_space.n\n",
    "input_shape = env.observation_space.shape\n",
    "net = DQN(input_shape, n_actions).to(device)\n",
    "optimizer = optim.Adam(net.parameters(), lr=0.00025)\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "# Training loop\n",
    "num_episodes = 1000\n",
    "epsilon = 1.0\n",
    "epsilon_decay = 0.995\n",
    "epsilon_min = 0.01\n",
    "batch_size = 32\n",
    "gamma = 0.99\n",
    "\n",
    "for episode in range(num_episodes):\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    total_reward = 0\n",
    "\n",
    "    while not done:\n",
    "        if np.random.random() < epsilon:\n",
    "            action = env.action_space.sample()\n",
    "        else:\n",
    "            state_tensor = torch.FloatTensor(state).unsqueeze(0).to(device)\n",
    "            q_values = net(state_tensor)\n",
    "            action = q_values.max(1)[1].item()\n",
    "\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "        total_reward += reward\n",
    "\n",
    "        # Store experience in memory (you might want to implement a replay buffer here)\n",
    "        \n",
    "        # Training step (you might want to implement experience replay here)\n",
    "        optimizer.zero_grad()\n",
    "        state_tensor = torch.FloatTensor(state).unsqueeze(0).to(device)\n",
    "        next_state_tensor = torch.FloatTensor(next_state).unsqueeze(0).to(device)\n",
    "        \n",
    "        current_q_values = net(state_tensor)\n",
    "        next_q_values = net(next_state_tensor)\n",
    "        \n",
    "        expected_q_values = current_q_values.clone()\n",
    "        expected_q_values[:, action] = reward + gamma * next_q_values.max(1)[0] * (1 - done)\n",
    "        \n",
    "        loss = criterion(current_q_values, expected_q_values)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        state = next_state\n",
    "\n",
    "    epsilon = max(epsilon_min, epsilon * epsilon_decay)\n",
    "    print(f\"Episode {episode + 1}, Total Reward: {total_reward}, Epsilon: {epsilon:.2f}\")\n",
    "\n",
    "# Save the trained model\n",
    "torch.save(net.state_dict(), \"mario_model.pth\")\n",
    "\n",
    "# Test the trained agent\n",
    "state = env.reset()\n",
    "done = False\n",
    "total_reward = 0\n",
    "\n",
    "while not done:\n",
    "    state_tensor = torch.FloatTensor(state).unsqueeze(0).to(device)\n",
    "    with torch.no_grad():\n",
    "        q_values = net(state_tensor)\n",
    "    action = q_values.max(1)[1].item()\n",
    "    state, reward, done, _ = env.step(action)\n",
    "    total_reward += reward\n",
    "    env.render()\n",
    "\n",
    "print(f\"Test Episode - Total Reward: {total_reward}\")\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\mini_conda\\envs\\collabkart\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 100, Total Reward: -39, Epsilon: 0.61\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 86\u001b[0m\n\u001b[0;32m     84\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m     85\u001b[0m     loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m---> 86\u001b[0m     \u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     88\u001b[0m     state \u001b[38;5;241m=\u001b[39m next_state\n\u001b[0;32m     90\u001b[0m epsilon \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmax\u001b[39m(epsilon_min, epsilon \u001b[38;5;241m*\u001b[39m epsilon_decay)\n",
      "File \u001b[1;32md:\\mini_conda\\envs\\collabkart\\Lib\\site-packages\\torch\\optim\\optimizer.py:373\u001b[0m, in \u001b[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    368\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    369\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m    370\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must return None or a tuple of (new_args, new_kwargs), but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    371\u001b[0m             )\n\u001b[1;32m--> 373\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    374\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_optimizer_step_code()\n\u001b[0;32m    376\u001b[0m \u001b[38;5;66;03m# call optimizer step post hooks\u001b[39;00m\n",
      "File \u001b[1;32md:\\mini_conda\\envs\\collabkart\\Lib\\site-packages\\torch\\optim\\optimizer.py:76\u001b[0m, in \u001b[0;36m_use_grad_for_differentiable.<locals>._use_grad\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m     74\u001b[0m     torch\u001b[38;5;241m.\u001b[39mset_grad_enabled(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdefaults[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdifferentiable\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m     75\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n\u001b[1;32m---> 76\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     77\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     78\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n",
      "File \u001b[1;32md:\\mini_conda\\envs\\collabkart\\Lib\\site-packages\\torch\\optim\\adam.py:163\u001b[0m, in \u001b[0;36mAdam.step\u001b[1;34m(self, closure)\u001b[0m\n\u001b[0;32m    152\u001b[0m     beta1, beta2 \u001b[38;5;241m=\u001b[39m group[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbetas\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m    154\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init_group(\n\u001b[0;32m    155\u001b[0m         group,\n\u001b[0;32m    156\u001b[0m         params_with_grad,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    160\u001b[0m         max_exp_avg_sqs,\n\u001b[0;32m    161\u001b[0m         state_steps)\n\u001b[1;32m--> 163\u001b[0m     \u001b[43madam\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    164\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparams_with_grad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    165\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    166\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    167\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    168\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    169\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    170\u001b[0m \u001b[43m        \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mamsgrad\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    171\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    172\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    173\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    174\u001b[0m \u001b[43m        \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mweight_decay\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    175\u001b[0m \u001b[43m        \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43meps\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    176\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmaximize\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    177\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforeach\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mforeach\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    178\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcapturable\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    179\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdifferentiable\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    180\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfused\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mfused\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    181\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgrad_scale\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    182\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfound_inf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    183\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    185\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\n",
      "File \u001b[1;32md:\\mini_conda\\envs\\collabkart\\Lib\\site-packages\\torch\\optim\\adam.py:311\u001b[0m, in \u001b[0;36madam\u001b[1;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, differentiable, fused, grad_scale, found_inf, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[0;32m    308\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    309\u001b[0m     func \u001b[38;5;241m=\u001b[39m _single_tensor_adam\n\u001b[1;32m--> 311\u001b[0m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    312\u001b[0m \u001b[43m     \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    313\u001b[0m \u001b[43m     \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    314\u001b[0m \u001b[43m     \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    315\u001b[0m \u001b[43m     \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    316\u001b[0m \u001b[43m     \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    317\u001b[0m \u001b[43m     \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mamsgrad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    318\u001b[0m \u001b[43m     \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    319\u001b[0m \u001b[43m     \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    320\u001b[0m \u001b[43m     \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    321\u001b[0m \u001b[43m     \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mweight_decay\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    322\u001b[0m \u001b[43m     \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    323\u001b[0m \u001b[43m     \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaximize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    324\u001b[0m \u001b[43m     \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcapturable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    325\u001b[0m \u001b[43m     \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdifferentiable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    326\u001b[0m \u001b[43m     \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgrad_scale\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    327\u001b[0m \u001b[43m     \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfound_inf\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\mini_conda\\envs\\collabkart\\Lib\\site-packages\\torch\\optim\\adam.py:432\u001b[0m, in \u001b[0;36m_single_tensor_adam\u001b[1;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, grad_scale, found_inf, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize, capturable, differentiable)\u001b[0m\n\u001b[0;32m    430\u001b[0m         denom \u001b[38;5;241m=\u001b[39m (max_exp_avg_sqs[i]\u001b[38;5;241m.\u001b[39msqrt() \u001b[38;5;241m/\u001b[39m bias_correction2_sqrt)\u001b[38;5;241m.\u001b[39madd_(eps)\n\u001b[0;32m    431\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 432\u001b[0m         denom \u001b[38;5;241m=\u001b[39m (\u001b[43mexp_avg_sq\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msqrt\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m/\u001b[39m bias_correction2_sqrt)\u001b[38;5;241m.\u001b[39madd_(eps)\n\u001b[0;32m    434\u001b[0m     param\u001b[38;5;241m.\u001b[39maddcdiv_(exp_avg, denom, value\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39mstep_size)\n\u001b[0;32m    436\u001b[0m \u001b[38;5;66;03m# Lastly, switch back to complex view\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import random\n",
    "\n",
    "# Define the grid world environment\n",
    "class GridWorld:\n",
    "    def __init__(self, size=5):\n",
    "        self.size = size\n",
    "        self.state = [0, 0]\n",
    "        self.goal = [size-1, size-1]\n",
    "\n",
    "    def reset(self):\n",
    "        self.state = [0, 0]\n",
    "        return self.state\n",
    "\n",
    "    def step(self, action):\n",
    "        # 0: up, 1: right, 2: down, 3: left\n",
    "        if action == 0 and self.state[0] > 0:\n",
    "            self.state[0] -= 1\n",
    "        elif action == 1 and self.state[1] < self.size - 1:\n",
    "            self.state[1] += 1\n",
    "        elif action == 2 and self.state[0] < self.size - 1:\n",
    "            self.state[0] += 1\n",
    "        elif action == 3 and self.state[1] > 0:\n",
    "            self.state[1] -= 1\n",
    "\n",
    "        done = (self.state == self.goal)\n",
    "        reward = 10 if done else -1\n",
    "        return self.state, reward, done\n",
    "\n",
    "# Define the Q-Network\n",
    "class QNetwork(nn.Module):\n",
    "    def __init__(self, input_size, output_size):\n",
    "        super(QNetwork, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, 64)\n",
    "        self.fc2 = nn.Linear(64, 64)\n",
    "        self.fc3 = nn.Linear(64, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        return self.fc3(x)\n",
    "\n",
    "# Set up the environment and the agent\n",
    "env = GridWorld(size=5)\n",
    "input_size = 2  # x and y coordinates\n",
    "output_size = 4  # 4 possible actions\n",
    "q_network = QNetwork(input_size, output_size)\n",
    "optimizer = optim.Adam(q_network.parameters(), lr=0.001)\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "# Training loop\n",
    "num_episodes = 1000\n",
    "epsilon = 1.0\n",
    "epsilon_decay = 0.995\n",
    "epsilon_min = 0.01\n",
    "gamma = 0.99\n",
    "\n",
    "for episode in range(num_episodes):\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    total_reward = 0\n",
    "\n",
    "    while not done:\n",
    "        if random.random() < epsilon:\n",
    "            action = random.randint(0, 3)\n",
    "        else:\n",
    "            with torch.no_grad():\n",
    "                q_values = q_network(torch.FloatTensor(state))\n",
    "                action = q_values.argmax().item()\n",
    "\n",
    "        next_state, reward, done = env.step(action)\n",
    "        total_reward += reward\n",
    "\n",
    "        # Q-learning update\n",
    "        q_values = q_network(torch.FloatTensor(state))\n",
    "        next_q_values = q_network(torch.FloatTensor(next_state))\n",
    "        target_q_value = reward + gamma * next_q_values.max() * (1 - done)\n",
    "\n",
    "        loss = criterion(q_values[action], target_q_value)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        state = next_state\n",
    "\n",
    "    epsilon = max(epsilon_min, epsilon * epsilon_decay)\n",
    "\n",
    "    if (episode + 1) % 100 == 0:\n",
    "        print(f\"Episode {episode + 1}, Total Reward: {total_reward}, Epsilon: {epsilon:.2f}\")\n",
    "\n",
    "# Test the trained agent\n",
    "state = env.reset()\n",
    "done = False\n",
    "total_reward = 0\n",
    "steps = 0\n",
    "\n",
    "print(\"\\nTesting the trained agent:\")\n",
    "while not done and steps < 100:\n",
    "    with torch.no_grad():\n",
    "        q_values = q_network(torch.FloatTensor(state))\n",
    "        action = q_values.argmax().item()\n",
    "    state, reward, done = env.step(action)\n",
    "    total_reward += reward\n",
    "    steps += 1\n",
    "    print(f\"Step {steps}: State {state}, Action {action}, Reward {reward}\")\n",
    "\n",
    "print(f\"\\nTest Episode - Total Reward: {total_reward}, Steps: {steps}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "collabkart",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
